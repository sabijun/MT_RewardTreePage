<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MT Reward Tree</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/MT_RewardTree_icon.png">

  <link rel="stylesheet" href="./static/css/index_mmmu.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script src="./static/js/index_emma.js"></script>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      bulmaCarousel.attach('#pdf-carousel', {
        slidesToScroll: 1,  
        slidesToShow: 1, 
        loop: true,         
        autoplay: true,     
        autoplaySpeed: 5000
      });
    });
  </script>
</head>
<body>




  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
  
            <!-- New Image -->
            <img src="./static/images/title-MT-RewardTree.png" alt="judge Logo" style="max-width: 60%; height: auto; margin-bottom: 1rem;">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://orcid.org/0000-0002-6396-3184">Zhaopeng Feng</a><sup>1</sup>,</span>              
              <span class="author-block">
                <a href="https://dske.zju.edu.cn/people_profile/?user_id=76">Jiahan Ren</a><sup>1</sup>,</span> 
              <span class="author-block">
                <a href="https://su-jiayuan.github.io/">Jiayuan Su</a><sup>1</sup>,</span>  
              <span class="author-block">
                <a href="https://dske.zju.edu.cn/people_profile/?user_id=80">Jiamei Zheng</a><sup>1</sup>,</span> 
              <span class="author-block">
                <a href="https://person.zju.edu.cn/en/lzz">Zhihang Tang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://zjui.intl.zju.edu.cn/en/node/778">Hongwei Wang</a><sup>1</sup>,</span> 
              <span class="author-block">
                <a href="https://person.zju.edu.cn/en/lzz">Zuozhu Liu</a><sup>1</sup>,</span> 
            </div>
  
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Zhejiang University - University of Illinois Urbana-Champaign Institute,</span>

              <span class="author-block"><sup>2</sup>Zhejiang University</span>
            </div>
  
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2411.15594"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href=""
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/sabijun/MT_RewardTree"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Github</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/collections/sabijun/mt-rewardtree-models-67cac935143f75dfae6f0938"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-solid fa-face-smiling-hands"></i>
                    </span>
                    <span>Hugging Face</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/collections/sabijun/mt-rewardtree-dataset-67cacadc0dcbc92c02428948"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-solid fa-table"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://x.com/will_wang_whc/status/1877550459290071318" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon has-text-white">üåê</span>
                    <span>Twitter</span>
                  </a>
                </span> -->
              </div>
            </div>
  
          </div>
        </div>
      </div>
    </div>
  </section>
  

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <!-- Ê∑ªÂä†ÂõæÁâá -->
      <img src="./static/images/MT-RewardTree_structure.png" alt="judge pipeline" style="max-width: 100%; height: auto;" id="pipeline_figure">
      <!-- ÂõæÁâáËØ¥ÊòéÊñáÂ≠ó -->
      <h2 class="subtitle has-text-centered">
      MT Reward Tree Structure.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">üîîNews</h2>
        <div class="content has-text-justified">
          <p>
            <b>üî• [2025-01-28]</b>: We added analysis on LLM-as-a-Judge and o1-like Reasoning Enhancement, as well as <b><a href="#meta-evaluation">meta-evaluation</a></b> results on <b>o1-mini</b>, <b>Gemini-2.0-Flash-Thinking-1219</b>, and <b>DeepSeek-R1</b>! 
          </p>
          <p>üåü <b>[2025-01-16]</b>: We shared and discussed the <b>methodologies</b>, <b>applications (Finance, RAG, and Synthetic Data)</b>, and future research directions of LLM-as-a-Judge at BAAI Talk! ü§ó  
            <br>
            <a href="https://event.baai.ac.cn/activities/878">[Replay]</a>  
            <a href="https://ticket-assets.baai.ac.c
            n/uploads/%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E5%BA%94%E7%94%A8%E4%B8%8E%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E6%8E%A2%E8%AE%A8.pdf">[Methodology]</a>  
            <a href="https://ticket-assets.baai.ac.cn/uploads/LLM-as-a-Judge-%E5%BE%90%E9%93%96%E6%99%8B.pdf">[RAG & Synthetic Data]</a>
            </p>            
          <p>
            <b>üöÄ [2024-11-23]</b>: WWe released <a href="https://arxiv.org/pdf/2411.15594">A Survey on LLM-as-a-Judge</a>, exploring LLMs as reliable, scalable evaluators and outlining key challenges and future directions!
          </p>
      </div> -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Process reward models (PRMs) have shown success in complex reasoning tasks for large language models (LLMs). 
            However, their application to machine translation (MT) remains underexplored due to the lack of systematic methodologies and evaluation benchmarks. To address this gap, we introduce MT-RewardTree, a comprehensive framework for constructing, evaluating, and deploying process reward models in MT. We propose a novel method for automatically generating high-quality token-level preference pairs using approximate Monte Carlo Tree Search (MCTS), mitigating the prohibitive cost of human annotation. Our framework establishes the first MT-specific reward model benchmark and provides a systematic comparison of different reward modeling architectures, revealing that token-level supervision effectively captures fine-grained preferences. 
          </p>  
          <p>
            Experimental results demonstrate that our MT-PRM-Qwen-2.5-3B achieves state-of-the-art performance in both token-level and sequence-level evaluation given the same input prefix. Furthermore, we showcase practical applications where PRMs enable test-time alignment for LLMs without additional training and significantly improve performance in hypothesis ensembling. Our work provides valuable insights into the role of reward models in MT research. Our code and data will be publicly available.
          </p>
          <!-- <p>
            Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. 
Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of "LLM-as-a-Judge," where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable and flexible assessments, LLMs present a <b>compelling alternative to traditional expert-driven evaluations. </b>
          </p>
          <p>
            However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization.
            This paper provides a comprehensive survey on LLM-as-a-Judge, offering a 
            <strong>formal definition</strong> and a <strong>detailed classification</strong>, while focusing on addressing the <b>core question</b>: 
            <u>How to build reliable LLM-as-a-Judge systems?</u> We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel <a href="#meta-evaluation">benchmark</a> designed for this purpose. 
            To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.
        </p>  -->
        </div>
        <!-- <img src="./static/images/paper_structure.png" alt="Motivation" style="width: 100%; max-width: 1000px; margin-top: 20px;"/> -->
      </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Token-level Preference Pairs Construction</h2>
        <div class="content has-text-justified">
          <p>
            We propose a token-centric approach that quantifies token quality based on its potential to contribute to higher-quality translations. This method aligns with Monte Carlo based PRMs construction techniques in mathematics, where step-wise quality is determined by its incremental contribution to deriving correct answers.
          </p>  
          <p>
            <b>Selection</b>:<br>The first phase involves selecting a portion of the existing tree that is most promising for further expansion. Starting from the root node, a standard approach would traverse the tree down to a leaf using the PUCT algorithm, and we select the existing prompt and previously generated tokens as the prefix y<sub>&lt;t</sub>.
          </p>
          <p>
            <b>Expansion</b>:<br>If the selected leaf node is not an EOS token, the node is expanded by generating k candidate children. In our paper, we select the top-2 candidate tokens that have the highest logits. These top-2 tokens, sharing the same prefix y<sub>&lt;t</sub>, form the basis for our token-level preference pair.
          </p>
          <p>
            <b>Simulation</b>:<br>From each expanded node a, we generate n complete translation rollouts until an EOS token is reached. We than evaluate the quality of the full translation sequence. These scores are averaged and further assigned as the value of node a.
          </p>
          <p>
            <b>Back-propagation</b>:<br>We compare the values of node a<sub>t1</sub> and a<sub>t2</sub> to determine which expanded token is superior. Finally, we retain the node with the higher value. This node, along with its corresponding prefix y<sub>&lt;t</sub>, is then used as the starting point in the next simulation cycle, beginning again at Step 1.
          </p>
        </div>
        <img src="./static/images/MCTS_process.png" alt="Motivation" style="width: 100%; max-width: 1000px; margin-top: 20px;"/>
      </div>
  </div>
</section>


<div id="meta-evaluation" class="container" style="margin-bottom: 2vh;">
  <!-- Benchmark. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Experimental Results</h2>
      <div class="content has-text-justified">
        <p>
          We also conducted a evaluation on our Qwen and Llama PRM model. Qwen PRM slightly outperforms LLaMA PRM in both translation tasks and across all metrics.
        </p>
      </div>
      <!-- <img src="./static/images/meta_eval_pipeline.png" alt="eval_pipeline" style="width: 80%; max-width: 1000px; margin-top: 20px;"/> -->
      <img src="./static/images/MT-RewardTree_results.png" alt="eval_results" style="width: 100%; max-width: 1000px; margin-top: 20px;"/>
      <img src="./static/images/Ensembeling.png" alt="eval_results" style="width: 100%; max-width: 1000px; margin-top: 20px;"/>
    </div>
  </div>
</div>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Practical Insights</h2>
        <div class="content has-text-justified">
          <p>
            <b>Test-time Alignment</b>:
          </p>  
          <p>
            Test-time alignment, also known as decoding-time alignment refers to the process of adjusting an LM‚Äôs output during inference to better align with human preferences, without additional training or fine-tuning. Its application in MT remains underexplored.<br>
            In the context of MT, given the prior context <i>s<sub>&lt;t</sub></i> and timestamp <i>t</i>, we define the reward-guided scoring function for a candidate token <i>a</i> as:
          </p>
          <p align="center">
            <i>s(a,s<sub>&lt;t</sub>)=LM(a|s<sub>&lt;t</sub>)+w¬∑P(r([s<sub>&lt;t</sub>,a]))</i>
          </p>
          <p>
            Compared to standard decoding strategies, this approach offers a more refined scoring function, as it encourages the generated text to: 1&rpar; Maintain semantic coherence and relevance with the prior context, and 2&rpar; Align more closely with reward-based criteria and human preferences. Test-time alignment also substantially reduces the need for the extensive resources typically required for LM alignment training.<br>
            We use Qwen2.5-14B-Instruct for generating tokens and leverage MT-PRM-LLaMA3.2-3B and MT-PRM-Qwen-2.5-3B as the models for providing token-level rewards. We randomly sample 500 cases from the WMT 2023 testset. The reward-guided decoding methods outperform the standard greedy decoding in both EN-RU and ZH-EN translation tasks, evaluated by the COMET, COMETKiwi and XCOMET-XL metrics. For instance, using the XCOMET-XL metric, LLaMA PRM and Qwen PRM outperform the standard greedy decoding by 17.5% and 17.9% in the EN-RU task respectively.
          </p>
          <img src="./static/images/Test-time Alignment.png" alt="Motivation" style="width: 100%; max-width: 1000px; margin-top: 20px;"/>
          <p>
            <b>Hypothesis Ensembling</b>:
          </p>
          <p>
            Test-time alignment, also known as decoding-time alignment refers to the process of adjusting an LM‚Äôs output during inference to better align with human preferences, without additional training or fine-tuning. Its application in MT remains underexplored.<br>
            In the context of MT, given the prior context <i>s<sub>&lt;t</sub></i> and timestamp <i>t</i>, we define the reward-guided scoring function for a candidate token <i>a</i> as:
          </p>
        </div>
        
      </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{gu2024survey,
      title={A Survey on LLM-as-a-Judge},
      author={Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others},
      journal={arXiv preprint arXiv:2411.15594},
      year={2024}
    }
  </code></pre>
  </div>
</section> -->

<div id="visitor-map" style="width: 300px; height: 300px; overflow: hidden; margin: 0 auto;">
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=ImmebrTlSgnLZ7WI_UYUKP2V9K5TeEPEHah2-Fc8W8A&cl=ffffff&w=a"></script>
</div>
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2411.15594">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/IDEA-FinAI/LLM-as-a-Judge" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
